---
title: "Accurate plant pathogen effector protein classification _ab initio_ with **deepredeff**, an ensemble of convolutional neural networks."
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  bookdown::pdf_document2:
    citation_package: natbib
    keep_tex: true
    fig_caption: true
    latex_engine: pdflatex
    template: svm_latex.tex
    toc: no
    includes:
      in_header: preamble.tex
fontsize: 12pt
fontfamily: sans
papersize: a4paper
author:
  - name: Ruth Kristianingsih, Dan MacLean
  - affiliation: The Sainsbury Laboratory, University of East Anglia, Norwich Research Park, Norwich, UK, NR4 7JJ, dan.maclean@tsl.ac.uk
abstract: "TBD."
geometry: margin=1in
biblio-style: apsr
bibliography: ["r-references.bib"]
graphics: yes
header-includes:
  - \usepackage{amsmath}
  - \usepackage{siunitx}
  - \usepackage{float}
---

```{r setup, include = FALSE}
library("papaja")
library(dplyr)
library(ggplot2)
library(patchwork)

# Source plotting functions
source("functions/plot_confusion_mat.R")
source("functions/plot_confusion_matrices.R")
source("functions/plot_acc_loss_history.R")
source("functions/get_lower_corr_mat.R")
source("functions/get_seqlogo.R")

# Build R pacakges Bib build
papaja::r_refs(file = "r-references.bib")

# ggplot2 global theme
theme_set(theme_bw())
```

```{r analysis-preferences, echo=FALSE}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(
  cache.extra = knitr::rand_seed,
  message = FALSE,
  echo = FALSE
)
```

# Introduction

Phytopathogens are a major threat to global crop production.  The fungal phytopathogen _Magnoporthe oryzae_ that causes cereal blast is responsible for around 30% of rice production loss and has now emerged as a pandemic problem on wheat [@Nalley:2016jp] The oomycete _Phytophthora infestans_ causes losses of around 6 billion USD to potato production, annually [@haas2009genome]. The bacterium _Ralstonia solanacearum_ has a wide host range and can cause loses of over 30% in potato, banana and groundnut [@Yuliar:2015hp]. The incidences of crop disease are increasing, global climate change and agricultural practice are expanding the geographical range of pathogens and upping the stakes in the evolutionary arms race.  Effector proteins are the shock troops of infection, manipulating the host at the infection interface to the pathogens advantage. Identifying and characterising a pathogen's effector content is a critical first step in understanding diseases and developing resistance,  but effectors are notoriously difficult to characterise from sequence data. In most phyla they have only a few easily determined sequence characteristics (some in fungi are cysteine rich or have a MAX motif, some in oomycetes have the RXLR motif or WY fold)  but in many cases no sequence identifiers are known [@Franceschetti:2017gc]. Characterising effectors requires painstaking molecular experimental work and genome-scale approaches have relied on complex computational pipelines with in-built _a priori_ assumptions about what might constitute an effector sequence in the absence of sequence features known to group them [@Sperschneider:2015kb]. To understand infection processes, to provide genome-level understanding of the functions of this important class of genes and to develop future disease resisting crop varieties there is a need to identify effectors computationally from genome and protein sequence data.

Machine learning (ML) algorithms are a general group of techniques most often used for classification of data into groups. Supervised ML require a set of training examples and associated data with which to learn. Defining the best data to use and collect, called feature selection is an important and difficult prerequisite.  ML approaches have been applied with success to biological sequence analysis, particularly in transcription factor binding site prediction, for the classification of eukaryote and bacterial nuclear proteins [@Savojardo:2017eq] and in the plant pathogen domain work by Sperschneider et al [@Sperschneider:2018gc] developed two ensemble-based machine learning models that could identify effectors and predict localisation with > 70% accuracy [@effectorp, @effectorp2].

Deep learning models are distinct from other machine learning processes in that pre-selection of important features is far less critical and the models can learn these features unsupervised from training data [@Jurtz:2017cr]. This property removes the need to know which properties of a data set must be examined before data collection begins. The Deep learning models can therefore classify on properties not necessarily known to the operator and could be used to uncover cryptic patterns in data. Convolutional neural networks (CNNs) are a type of neural network that have found wide application in numerous machine vision problems, including image object classification and facial identification [@krizhevsky2012, @lawrence1997], in time-series data analysis [@pyrkov2018] and natural language processing [@collobert2008]. In the biomedical domain they have been used in drug discovery [@wallach2015] and gene network prediction [@maclean2019]. In studies with bacterial type III secreted effectors Xue et al developed an accurate CNN classifier for bacterial sequences [@deept3]. CNNs encode information about the features used to classify that can be extracted and interpreted. In a sequence classification problems this means they have the potential to reveal novel sequence features that other bioinformatics approaches have not and could be of particular utility when analysing sets of effectors.

Deep learning approaches require positive and negative examples from which to learn - here a list of sequences annotated as an effector or not. The larger and more accurate the list the more sensitivity a model can obtain. It is critical that training examples are experimentally verified effectors. Much of the effector annotation in public genomics databases is from computational predictions of genomics and is therefore of experimentally unverified hypothetical effectors. A good source of experimentally verified data is in the Molecular Plant Microbe Interactions (MPMI) literature and The widest ranging manual curation of MPMI papers is being performed as part of the PHI-Base [@phibase] database strategy, PHI-base is an expertly curated database of genes proven experimentally to affect the outcome of pathogen host interactions and is therefore an excellent source of reliable effector sequences.

Here we use combinations of CNNs that do not rely on _a priori_ feature selection to classify experimentally verified effectors and non-effectors in three taxa of plant pathogen: bacterial, fungal and oomycete. We show that these have very strong predictive power, can outperform existing effector prediction methods in accuracy and a better balance of sensitivity and specificity.  We also analyse the activations of the models in response to effectors and non-effectors to gain insights into the sequence features that are allowing classification. We have produced an R package that will allow other scientists to easily classify their own sequences of interest.


# Methods

## Sequence Data Collection

Sequence data were collected from the PHI-Base database version 4.8 [@phibase] by accessing a text dump of the data prepared on request by the PHI-Base team, the file can be accessed at [https://github.com/PHI-base/data/blob/master/releases/phi-base_current.csv](https://github.com/PHI-base/data/blob/master/releases/phi-base_current.csv). The pipeline in Figure \@ref(fig:effectorcollection) outlines the steps used. We filtered plant effector proteins and their taxonomic groups and collected sequences from UniProt Release 2019_05, using the code in [https://github.com/TeamMacLean/ruth-effectors-prediction/blob/master/scripts/r-scripts/getting-data-new/binary-class/0001_first_step_getting_data.Rmd](https://github.com/TeamMacLean/ruth-effectors-prediction/blob/master/scripts/r-scripts/getting-data-new/binary-class/0001_first_step_getting_data.Rmd). We created a correspondingly sized data set of non-effectors with secretion signals originating in species matched to those from which the effectors were drawn. We downloaded sequences for randomly selected proteins matching these criteria from Ensembl databases [@yates2020] : specifically Ensembl Fungi, Protists and Bacteria manually using the BioMart tools [@smedleybiomart2009]. Since the BioMart tool is not available on Ensembl Bacteria, we downloaded whole proteome protein sequnces from species matched to those from which the effector came using FTP. With these we used SignalP 3.0 [@dyrlovbendtsen2004] in order to filter the secreted sequences and selected accordingly. We used default paramaters from SignalP, except the type of organism group which is `euk` for both fungi and oomycete sequences, and `gram-` or `gram+` for bacteria sequences. Redundant sequences were filtered using BLASTp [@camachoblast2009]. We achieved these steps using the code in [https://github.com/TeamMacLean/ruth-effectors-prediction/blob/master/scripts/r-scripts/getting-secreted-data/0005_process_signalp_data.Rmd](https://github.com/TeamMacLean/ruth-effectors-prediction/blob/master/scripts/r-scripts/getting-secreted-data/0005_process_signalp_data.Rmd).

## Encoding and Subsetting Sequences

The sequences collected were encoded using either one-hot encoding (CNN-LSTM based models) or integer based encoding (CNN-GRU-LSTM models). Sequences were post-padded with zeroes to bring the vectors to identical lengths to each other and the longest sequence in the taxon data set. The longest sequence  for bacteria, fungi, and oomycete are 2574, 4034, and 934, respectively. Encoded sequences were split into taxon specific training, test and validation sets at a 60%, 20%, 20% split respectively as described in code at [https://github.com/TeamMacLean/ruth-effectors-prediction/blob/master/scripts/r-scripts/getting-secreted-data/0008_split_and_encode.Rmd](https://github.com/TeamMacLean/ruth-effectors-prediction/blob/master/scripts/r-scripts/getting-secreted-data/0008_split_and_encode.Rmd)


## Model Training

We trained four model types on each taxon specific sequence set: CNN-LSTM, CNN-GRU, LSTM-Embedding,  GRU-Embedding. We trained each model using a basic random hyperparameter setting initialisation step followed by hyperparamter scans. All models were implemented in Python 3.6.9 using the deep learning API Keras 2.2.4 with Tensorflow 1.12.0 backend, using GPU NVIDIA GK110GL Tesla K20c and CPU AMD Opteron(TM) Processor 6272 with memory 128 GB. 


### Hyperparameter Scans

Hyperparameter scans were performed using random search, a hyperparameter optimization method which 
implements a randomized search over hyperparameters, where each hyperparameter setting is randomly sampled from a distribution over possible hyperparameter values [@Bergstra2012]. In particularly, we used RandomSearchCV(), the implementation of random search method in `scikit-learn 0.19.2` [@scikit-learn] together with KerasClassifier() which is implementation of the scikit-learn classifier API for keras. Code for this can be found in [https://github.com/TeamMacLean/ruth-effectors-prediction/tree/master/scripts/python-scripts/hyperparameter-scan-scripts](https://github.com/TeamMacLean/ruth-effectors-prediction/tree/master/scripts/python-scripts/hyperparameter-scan-scripts). All model training was performed as described in section Model Training.

### Fine tuning

Fine tuning was performed manually using `keras 2.2.4` together with metrics module and KFold cross validator from `scikit-learn 0.19.2`. Scripts implementing the tuning can be found at  [https://github.com/TeamMacLean/ruth-effectors-prediction/tree/master/scripts/python-scripts/manual_tune_scripts](https://github.com/TeamMacLean/ruth-effectors-prediction/tree/master/scripts/python-scripts/manual_tune_scripts). 

### Model Classification Correlation

We calculated correlations between the classifications from best performing models on the hold-out test data set using Pearson's correlation co-efficient on the 1/0 classification vectors.

## Ensemble Functions

We computed an aggregate classification using two different ensemble functions, weighted average and an overall majority option.

Weighted average is computed as

\begin{equation}
\bar{y} = \frac{\sum_{i=1}^{n} w_i y_i}{\sum_{i=1}^{n} w_i} ,
\end{equation}

where $w_i$ is the weight, $y_i$ is the prediction value of the $i^{th}$ model, and $n$ is the total number of model. In our case, we use the accuracy of each model as the average.

Overall majority is computed as

\begin{equation}
\tilde{y} = argmax(x_1, x_2, ..., x_n) ,
\end{equation}
where

\begin{equation}
  y =
  \begin{cases}
     1 & \mbox{for } \tilde{y} > 0.5 \\
     0 & \mbox{for } \tilde{y} \leq 0.5
  \end{cases}
\end{equation}


## Metrics

We used the following calculations for different accuracy metrics in our evaluations, specifically: accuracy, sensitivity, specificity. $TP$, $TN$, $FP$, and $FN$ refer to the number of true positives, true negatives, false positives and false negatives, respectively.

Accuracy ($Acc$) is the ratio between correctly classified non-effectors and effectors and all samples:
\begin{equation}
  Acc = \frac{TP + TN}{TP + TN + FP + FN}
\end{equation}

Sensitivity $Sn$ is the ratio between correctly predicted as effectors and all effectors:
\begin{equation}
  Sn = \frac{TP}{TP + FN}
\end{equation}

Specificity ($Sp$) is the ratio between correctly predicted as non-effectors and all non-effectors:
\begin{equation}
  Sp = \frac{TN}{TN + FP}
\end{equation}

F1-score is the harmonic average of the precision and recall:
\begin{equation}
  F_{1} = \frac{2TP}{2TP + FN + FP}
\end{equation}

## Activation Map Analysis

To visualise the regions of the sequences that the models were using to discriminate between effector and non-effector we adapted a Grad-CAM (Gradient-weighted Class Activation Mapping) approach [@Selvaraju2019]. We extracted the convolutional layer `convd_1` and computed the gradient of the model output with respect to the output of `convd_1`. The feature map was weighted by every channel and mean result taken. All activation maps were summed, normalised and smoothed using Fourier analysis. We used `tensorflow 1.12.0` to compute the activation maps and discrete fourier transform (numpy.fft) from `numpy 1.17.3` to smooth the result. The code we used for computing and visualising these heatmaps can be found at [https://github.com/TeamMacLean/ruth-effectors-prediction/tree/master/scripts/python-scripts/heatmaps](https://github.com/TeamMacLean/ruth-effectors-prediction/tree/master/scripts/python-scripts/heatmaps) .

## Effector Prediction Software and Training Data

To test the performance of our models against commonly used tools we used DeepT3 version 1 [@deept3] and EffectiveT3 version 1.0.1 [@effectivet3] for bacterial sequences. Effector P Version 1.0 and 2.0 [@effectorp, @effectorp2] for fungal sequences and EffectR [@effectR] for oomycete sequences. All the models publish the datasets used to train the models or some examples. We used positive training examples (effector sequence examples) in the comparisons we performed. EffectorP provides three different positive datasets (training, test and hold-out validation) for EffectorP 1.0 and 2.0 at [http://effectorp.csiro.au./data.html](http://effectorp.csiro.au./data.html). EffectiveT3 provides a training set at [https://effectors.csb.univie.ac.at/sites/eff/files/others/TTSS_positive_training.faa](https://effectors.csb.univie.ac.at/sites/eff/files/others/TTSS_positive_training.faa). DeepT3 provides three sets, a non redundant Pseudomonas syringae effector dataset, a training dataset and a test data set at [https://github.com/lje00006/DeepT3/tree/master/DeepT3/DeepT3-Keras/data](https://github.com/lje00006/DeepT3/tree/master/DeepT3/DeepT3-Keras/data). EffectR uses 6 RXLR oomycete sequences described in [@haas2009genome] as examples rather than as a training set, namely *PexRD36*, *PexRD1*, *ipi01/Avrblb1*, *Avr1*, *Avr4*, and *Avr3a* . All these sets were used in their respective tools with default settings.   



# Results

## Sequence Collection

The performance of the trained classifiers is dependent on the quality of the input training data, so it was important that we collected as high a quality set of annotated effectors as possible. To this end we used PHI-Base [@phibase] as our primary sequence origin. Sequences in PHI-Base are human curated from the literature and have therefore been noted in experimental studies. They do not derive from large scale annotations or contain hypothetical or predicted proteins. This attribute makes it ideal for our purposes as the effectors in PHI-Base are those that have been specifically reported as such in the published literature and are not of the class of sequences that are merely suspected of being effectors on the basis of carrying a secretion signal. To collect effector sequences we parsed a whole database text dump of version 4.8 [https://github.com/PHI-base/data](https://github.com/PHI-base/data/blob/master/releases/phi-base_current.csv), all proteins marked as plant pathogen effectors were filtered and we used the IDs and UniProt IDs to collect the protein sequences from PHI-Base or UniProt if PHI-Base stored only the ID. The sequences and IDs retrieved can be seen in the data file in this manuscript's repository [https://github.com/TeamMacLean/ruth-effectors-prediction/blob/master/data/getting-data-new/binary-class-data/effector_data.csv](https://github.com/TeamMacLean/ruth-effectors-prediction/blob/master/data/getting-data-new/binary-class-data/effector_data.csv). Effector sequences were then divided into taxonomic groups as bacterial, fungal or oomycete derived accordingly. In total 190 bacterial effectors from 13 species were collected, 97 fungal effectors from 16 species were collected and 85 oomycete effectors from 6 species were collected (Table \@ref(tab:effsummary) ). The species and effector count in each group can be seen in Tables \@ref(tab:bacefftable), \@ref(tab:fungiefftable) and \@ref(tab:oomefftable).

Sequences for non-effector, secreted proteins were collected using a similar pipeline. Randomly selected proteins from each species carrying secretion signals were extracted from Ensembl databases using the BioMart tool. For each species noted in Tables \@ref(tab:bacefftable), \@ref(tab:fungiefftable) and \@ref(tab:oomefftable) we collected from either the same strain or species an identical number of non-effector, secreted proteins to that in the effector set. This gave us a balanced data set of effector proteins as positive learning examples and non-effector secreted proteins as negative learning examples. Figure \@ref(fig:noneffectorcollection) summarises the process of building the non-effector set, and the full set of sequences and IDs retrieved can be seen in the following data file [https://github.com/TeamMacLean/ruth-effectors-prediction/tree/master/data/secreted_data](https://github.com/TeamMacLean/ruth-effectors-prediction/tree/master/data/secreted_data).


```{r effectorcollection, fig.width=10, fig.height=10, out.width='50%', fig.align='center', fig.cap="Workflow diagram for collection of effector sequences from the PHI-Base database annotation and cross reference to UniProt"}
knitr::include_graphics(
  "figures/flowchart_effectorsequence_collection.pdf"
)
```

```{r noneffectorcollection, fig.width=10, fig.height=10, fig.cap="Workflow diagram for collection of secreted non-effector sequences from Ensembl Bacteria, Fungi, and Protists"}
knitr::include_graphics(
  "figures/flowchart_secreted_noneffectorsequence_collection.pdf"
)
```

```{r, effsummary}
apa_table(
  readr::read_tsv("tables/effectors_by_path_group.tab"),
  digits = 0,
  align = 'c',
  caption = 'Count of effectors listed in publications curated by PHI-Base used in this study in three major plant pathogen groups'
)
```


## Model Selection and Training
In order to identify a useful classifier we took a randomised hyperparamter search over some likely base model architectures. We selected four base architectures on which to build models for learning. Two of these contained Convolutional Neural Network (CNN) layers followed by either a Long Short Term Memory Layer (LSTM) or a Gated Recurrent Unit (GRU), two contained an Embedding Layer followed by the LSTM or GRU. All models had fully-connected dense layers after this. See Figure \@ref(fig:modelarch).

We defined a range of values for the hyperparameters that could be optimised in each architecture, 10 for CNN-LSTM, 12 for CNN-GRU, 9 for LSTM-Embedding and 9 for GRU-Embedding. To test all combinations of values in these ranges would take a prohibitive amount of processor time, so we selected 50 sets of values for each model in each taxon at random to start training, `r 3 * 4 * 50 * 5` models in total. ^[For each architecture, 50 models are selected randomly. Therefore there are 3 taxon * 4 architecture * 50 models * 5 cross validation]  Model variants within the hyperparameter search were assessed by comparing accuracy values on the development validation fraction of the training data. Other hyperparameters were fixed and are listed in Table \@ref(tab:fixedhyperparam). For each model type and taxon training data combination we selected the hyperparameter set giving highest accuracy on the validation fraction. ^[\@ref(tab:possiblecombination) is table of total number of all hyperparameter settings in the whole hyperparameter space for each architecture] From this we had twelve candidate models to develop further. 

```{r modelarch, fig.width=10, fig.height=10, fig.cap="Overview of model architectures tested in this analysis, A: CNN-LSTM model; B: CNN-GRU model; C: LSTM-Embedding model; D: GRU-Embedding model"}
knitr::include_graphics("figures/model_arch.pdf")

```

```{r, acclossfinal}
 readr::read_tsv("tables/acc_loss_final.tab") %>%
  apa_table(            digits = 3,
  align = 'c',
  caption = 'Accuracy and loss values from best performing parameters values for each model'
         )
```

We then manually ran and checked the accuracy and loss of the twelve models on the training and validation sets to investigate instances of overfitting and assess generality. Smaller models are less likely to overfit data, so we investigated the effect of regularization rate, filter count and kernel size on the progress and accuracy of the model as we reduced the size. Parameters varied in this phase are listed in \@ref(tab:tunedhyperparam). Final selected hyperparameter settings for models in each taxon can be seen in Table \@ref(tab:hyperparam). The values of accuracy and loss of each model produced are shown in Table \@ref(tab:acclossfinal). We found that by reducing the number of kernels on all models from 2 to 1 and the number of filters reduced from 32 to 16 we removed apparent overfitting and retained high accuracy, with training completing in 40 epochs.

Final training progressions for each model in each taxon can be seen in Figure \@ref(fig:trainingprogress). We tested the finalised models on the hold-out test fraction of the data that had not been previously seen, for the four bacterial sequence trained models we had accuracies in the range 93.4  to 97.4, for the four fungal models we observed accuracy in the range  60.5 to 84.2 and for the four oomycete models we saw accuracy from 64.7 to 82.3, reported in Fig \@ref(fig:overallmodel). All the models we generated had high statistical power and can accurately and reliably classify effectors from other secreted proteins in that taxon.

The final twelve models were saved into HDF5 objects and stored in the repository at [https://github.com/TeamMacLean/ruth-effectors-prediction/tree/master/data/final_model_hdf5](https://github.com/TeamMacLean/ruth-effectors-prediction/tree/master/data/final_model_hdf5).

```{r trainingprogress, fig.height= 10, fig.width= 10, fig.cap="Training trajectories showing Accuracy and Loss over 40 epochs for four model types on A: Bacterial sequence training set; B: Fungal sequence training set; C: Oomycete sequence training set"}
# knitr::include_graphics("figures/final_train_progress.png")

bacteria_acc_loss <- plot_acc_loss_history(
  train_data = data.table::fread("data/processed/bacteria_hist_train.csv"),
  val_data = data.table::fread("data/processed/bacteria_hist_val.csv"),
  plot_tag = "A"
) + ggplot2::theme(legend.position = "none", axis.title.x=element_blank())

fungi_acc_loss <- plot_acc_loss_history(
  train_data = data.table::fread("data/processed/fungi_hist_train.csv"),
  val_data = data.table::fread("data/processed/fungi_hist_val.csv"),
  plot_tag = "B"
) +
  ggplot2::theme(strip.background.x = element_blank(),
   strip.text.x = element_blank(),
    axis.title.x = element_blank())

oomycete_acc_loss <- plot_acc_loss_history(
  train_data = data.table::fread("data/processed/oomycete_hist_train.csv"),
  val_data = data.table::fread("data/processed/oomycete_hist_val.csv"),
  plot_tag = "C"
) +
  ggplot2::theme(legend.position = "none") +
  ggplot2::theme(strip.background.x = element_blank(),
   strip.text.x = element_blank())


(bacteria_acc_loss + fungi_acc_loss + oomycete_acc_loss) +
  plot_layout(nrow = 3, heights = c(3, 3, 3))
```

```{r overallmodel, fig.cap="Final performance of models in each taxon on the unseen, hold-out test data fraction of sequences."}
# apa_table(
#   readr::read_tsv("tables/final_model_performance.tab"),
#   digits = 3,
#   align = 'c',
#   caption = 'Final performance of models in each taxon on the unseen, hold-out test data fraction of sequences.'
# )
readr::read_tsv("tables/final_model_performance_tidy.tab") %>%
  ggplot2::ggplot() +
  ggplot2::aes(
    x = Model,
    y = Accuracy,
    color = Accuracy
  ) +
  ggplot2::geom_point() +
  ggplot2::scale_color_viridis_c(begin = 0.65, end = 0.1) +
  ggplot2::facet_wrap(~ Taxon, scales = "free_x") +
  ggplot2::theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

## Model Characteristics

We examined the tendency of the models to call false positives or false negatives preferentially by creating confusion matrices of the classifications relative to the ground truth on the hold-out test data. The bacterial sequence trained models in general showed high accuracy and only one or two miscalls with no error bias except for the GRU-Embedding model which called five from 38 effectors as non effectors. The fungal sequence trained models were less accurate overall and showed a small amount more bias, again in the GRU-Embedding model, which was biased towards calling effectors as non-effectors and the CNN-LSTM model which was slightly biased in the opposite direction, calling non-effectors as effectors. The oomycete models were again quite balanced but the GRU-Embedding model showed a quite conservative tendancy calling 12 out of 17 effectors as non-effectors whilst getting all 17 non-effectors correct. Overall the models are accurate and show little to no bias toward false positive or false negatives, with the exception of the GRU-Embedding type. In oomycete sequences in particular and in this class of model across the different sequence types showed itself to tend to call real effectors as not of that class.

Classification correlations between the different model architectures were high and positive in the bacterial sequence trained model's calls, in the range 0.8 to 0.88, see Figure \@ref(fig:confusionmodelcorrs). CNN-GRU and LSTM-Embedding showed identical prediction sets. We saw similar levels of correlation  in the CNN-LSTM, GRU-Embedding and CNN-GRU fungal sequence trained model, in the range 0.79 to 0.9 ; though there was a significantly lower range of correlations with the LSTM-Embedding which were in the range 0.36 to 0.51. The models trained on oomycete sequences all showed this lower range of correlations, in the range 0.30 to 0.65. The higher correlation across bacterial trained models is likely from a mixture of the larger training set size and a greater uniformity of the sequences themselves. For the fungal sequence trained models we can see that the LSTM-Embedding model does not perform as well as the others. The oomycete sequence trained models all show a lower range correlation reflecting the likely less uniform and smaller training set. It is clear that, particularly for the fungal and oomycete models, each architecture is capturing separate aspects of the sequences and classifying on those with slightly varying levels of success.

```{r confusionmodelcorrs, fig.height = 8, fig.width = 8, fig.cap = "Confusion matrices showing tendency of models to call false positive or false negative errors on the final test data; and Pearson correlations between classifications made by different models. A: Models trained on bacterial sequences; B: Models trained on fungal sequences; C: Models trained on oomycete sequences.", message = FALSE, echo=FALSE}

# Load data
bacteria_ensemble_results <- data.table::fread("data/raw/bacteria_pred_ens.csv", drop = "V1")

bacteria_test_label <- data.table::fread("data/raw/bacteria_testing.csv") %>%
  dplyr::select(label) %>%
  dplyr::mutate(label = as.factor(label))

oomycete_ensemble_results <- data.table::fread("data/raw/oomycete_pred_ens.csv",drop = "V1")

oomycete_test_label <- data.table::fread("data/raw/oomycete_testing.csv") %>%
  dplyr::select(label)  %>%
  dplyr::mutate(label = as.factor(label))

fungi_ensemble_results <- data.table::fread("data/raw/fungi_pred_ens.csv", drop = "V1")

fungi_test_label <- data.table::fread("data/raw/fungi_testing.csv") %>%
  dplyr::select(label)  %>%
  dplyr::mutate(label = as.factor(label))

# Correlation plots
corr_bacteria <- get_lower_corr_mat(bacteria_ensemble_results)
corr_fungi <- get_lower_corr_mat(fungi_ensemble_results)
corr_oomycete <- get_lower_corr_mat(oomycete_ensemble_results)

# Confusion matrices
conf_bacteria <- plot_confusion_matrices(
  data = bacteria_ensemble_results,
  true_label = bacteria_test_label,
  model_list = c("cnn_lstm", "cnn_gru", "gru_emb", "lstm_emb"),#, "ensemble_weighted", "ensemble_voting")
  fill_upper_limit = NA
) +
  ggplot2::labs(tag = "A")

conf_fungi <- plot_confusion_matrices(
  data = fungi_ensemble_results,
  true_label = fungi_test_label,
  model_list = c("cnn_lstm", "cnn_gru", "gru_emb", "lstm_emb"),#, "ensemble_weighted", "ensemble_voting")
  fill_upper_limit = NA
) +
  ggplot2::labs(tag = "B")

conf_oomycete <- plot_confusion_matrices(
  data = oomycete_ensemble_results,
  true_label = oomycete_test_label,
  model_list = c("cnn_lstm", "cnn_gru", "gru_emb", "lstm_emb"),#, "ensemble_weighted", "ensemble_voting")
  fill_upper_limit = NA
) +
  ggplot2::labs(tag = "C")

# Patchwork
corr_patch <- (
  conf_bacteria + corr_bacteria +
    plot_layout(ncol = 2, widths = c(3,1), tag_level = "new")
  ) /
  (
    conf_fungi +  corr_fungi +
      plot_layout(ncol = 2, widths = c(3,1), tag_level = "new")
  ) /
  (
    conf_oomycete + corr_oomycete +
      plot_layout(ncol = 2, widths = c(3,1), tag_level = "new")
  )

corr_patch +
  plot_annotation(tag_levels = c("A"))
```

## Ensemble Models

We examined the usefulness of combining the predictions of the different model architectures using an ensemble function that takes the vectors of classifications of each model architecture as input. We performed the classification of the hold-out test data set using the ensembled models and the results can be seen in Figure \@ref(fig:ensembleresults). With the models trained in bacterial sequences we saw an increase in classification accuracy over the best model, up to 0.99 for both ensemble functions. However, with the fungal and oomycete models we saw decreases relative to the best single model in both cases due to the higher accuracy of the CNN-LSTM model being diluted by the combined inacuracy of the other model architectures. Examining the overlaps in classifications between the CNN-LSTM and CNN-GRU/LSTM-Embedding respectively showed that the two lesser performing models were not simply predicting subsets of the CNN-LSTM model, in both cases the lesser models were able to identify three effectors correctly that were missed by the generally stronger models. This indicates that the weaker models may be classifying on some patterns missed by the CNN-LSTM model.

```{r ensembleresults, fig.width=8, fig.height=4, fig.cap="Final performance of Ensemble models in each taxon on the unseen, hold-out test data fraction of sequences."}
# apa_table(
#   readr::read_tsv("tables/ensemble_model_performance.tab"),
#   digits = 3,
#   align = 'c',
#   caption = 'Final performance of Ensemble models in each taxon on the unseen, hold-out test data fraction of sequences.'
readr::read_tsv("tables/ensemble_model_performance_tidy.tab") %>%
  ggplot2::ggplot() +
  ggplot2::aes(x = Combination, y = Accuracy) +
  ggplot2::geom_point(ggplot2::aes(colour = EnsembleFunction,
                                   shape = EnsembleFunction), size = 2) +
  ggplot2::scale_shape_manual(values = c("Ensemble Voting" = 19, "Ensemble Weighted" = 17)) +
  ggplot2::scale_color_viridis_d(begin = 0.65, end = 0.1) +
  ggplot2::labs(color = "Ensemble Function", shape = "Ensemble Function") +
  ggplot2::facet_wrap(~ Taxon, scales = "free_x") +
  ggplot2::theme(axis.text.x = ggplot2::element_text(angle = 45, hjust = 1))
```

## Comparison With Other Classification Software

Given the accuracies of the above we selected the ensemble bacterial model and the CNN-LSTM fungal and oomycete models to evaluate the performance of our models against widely used effector identification software. We compared against predictions from the bacterial effector prediction programs DeepT3 [@deept3] and EffectiveT3 classification module for plant-associated bacteria 1.0.1 [@effectivet3], the fungal effector prediction programs EffectorP 1.0 and 2.0 [@effectorp, @effectorp2] and the oomycete effector predictor EffectR [@effectR]. Each comparison was carried out using the respective hold-out test sequence set for each taxon. For all taxon we saw greater Accuracy and F1 scores from our models than the established software, as shown in Figure \@ref(fig:vsothersmetrics). This was particularly marked in the F1 score, which incorporates a measure of the incorrect calls. Absolute improvements were up to 15 %  in bacterial sequences, 15 % in fungal sequences and 20 %  in the oomycete sequences. The confusion matrices in Figure \@ref(fig:vsotherscfmat) show that accuracy and F1 score was compromised in all the established tools by the tendency of them all to misclassify true effectors as not effectors.  All the established software classifiers we tested show lower sensitivity than the models we have developed here. 

We also evaluated the deep learning models we have developed on the training and hold-out validation sequences used to train the previous methods. We calculated the proportion of the effectors in the training set that the tools could find on their respective training and validation sets, according to availability. The bacterial tools EffectiveT3 and DeepT3 showed lower proportion found than our Ensemble Weighted model, as in Figure \@ref(fig:owndata)A, consistent with the observation that our Ensemble Weighted model performed more strongly on the validation set that we generated. Interestingly, both versions of EffectorP found a greater proportion of the effectors in the EffectorP provided training sets than our CNN-LSTM model, but in the unseen validation data provided with EffectorP 2, all three models performed identically (Figure \@ref(fig:owndata)B ). The EffectorP 1 and 2 scores on validation data are well below the scores for the training data, a result that is usually interpreted as being evidence of an overfitted and less generalisable model. Our CNN-LSTM model for fungi showed similar scores across the training and validation set indicating a greater generability and equivalent power. Only six effector sequences are provided as examples with the oomycete specific effector finder effectR. As this is not a trainable model in the same sense as the others, no large training set is needed. We attempted classification with these and effectR was able to classify 5 of the 6, whereas our CNN-LSTM model for oomycetes classified 3 of the 6 ( Figure \@ref(fig:owndata)C ).

Overall, our models performed more strongly than the previously available ones tested across the range of sequences examined.

```{r vsothersmetrics, fig.width=8, fig.height=6, fig.cap="Accuracy and F1 score of the classifications made by our models compared with those made by widely used software", fig.pos="H"}
data.table::fread("data/processed/acc_f1score_vs_othertools.csv") %>%
  dplyr::mutate(model = case_when(
    model == "ensemble_weighted" ~ "Ensemble Weighted",
    model == "cnn_lstm" ~ "CNN-LSTM",
    model == "deepT3" ~ "DeepT3",
    model == "effectiveT3" ~ "EffectiveT3",
    model == "effectorp_1" ~ "EffectorP 1",
    model == "effectorp_2" ~ "EffectorP 2",
    TRUE ~ "EffectR"
  )) %>%
  dplyr::mutate(
    model = factor(model, levels = c("Ensemble Weighted", "DeepT3", "EffectiveT3",
                                    "CNN-LSTM", "EffectorP 1", "EffectorP 2",
                                    "EffectR"))
  ) %>%
  ggplot2::ggplot() +
  ggplot2::aes(
    x = model,
    y = .estimate,
    color = .metric,
    shape = .metric
  ) +
  ggplot2::scale_shape_manual(values = c("Accuracy" = 19, "F1 Score" = 17)) +
  ggplot2::geom_point(size = 2) +
  ggplot2::facet_wrap(~groups, nrow = 1, scales = "free_x") +
  ggplot2::scale_color_viridis_d(begin = 0.65, end = 0.1) +
  ggplot2::labs(x = "Classifier", y = "Score", color = "Metric", shape = "Metric") +
  ggplot2::theme(axis.text.x = ggplot2::element_text(angle = 45, hjust = 1))
```

```{r vsotherscfmat, fig.width=8, fig.height=8, fig.cap="Confusion matrices of classifications made by our models compared with those made by widely used software. A: Tools evaluated on bacterial sequences; B: Tools evaluated on fungal sequences; C: Tools evaluated on oomycete sequences.", fig.pos="H"}

bacteria_confusion_mat_df <- readr::read_csv("data/processed/bacteria_confusion_mat_df.csv")
fungi_confusion_mat_df <- readr::read_csv("data/processed/fungi_confusion_mat_df.csv")
oomycete_confusion_mat_df <- readr::read_csv("data/processed/oomycete_confusion_mat_df.csv")

bacteria_conf <- plot_confusion_mat(
  bacteria_confusion_mat_df,
  model_list = c("ensemble_weighted", "deepT3", "effectiveT3"),
  fill_upper_limit = 40
) + ggplot2::theme(legend.position = "none")

fungi_conf <- plot_confusion_mat(
  fungi_confusion_mat_df,
  model_list = c("cnn_lstm", "effectorp_1", "effectorp_2"),
  fill_upper_limit = 40
)

oomycete_conf <- plot_confusion_mat(
  oomycete_confusion_mat_df,
  model_list = c("cnn_lstm", "effectR"),
  fill_upper_limit = 40
) + ggplot2::theme(legend.position = "none")

(bacteria_conf + fungi_conf + oomycete_conf) +
  plot_layout(ncol = 1, heights = c(1,1,1)) +
  plot_annotation(tag_levels = "A")
```

```{r, owndata, fig.height = 8, fig.cap="Performance of models on training and validation datasets used to train other effector prediction tools. A: Tools trained on bacterial sequences; B: Tools trained on fungal sequences; C: Tools trained on oomycete sequences."}
d <- readr::read_csv(here::here("data","processed", "owndatasets.csv"))  
  
  mkplot <- function(df, which_tax ) {
    df$Tool <- factor(df$Tool, levels = c("cnn_lstm", "ens_weight", "Effective T3 All", "Effective T3 Plants", "Deep T3", "EffectorP", "EffectorP 2", "effectR"))
    p <- df %>% filter(Taxon == which_tax) %>% 
     ggplot() +
    aes(Tool, `Proportion Found`) +
    geom_bar(stat= 'identity', fill = rgb(52/255, 106/255, 141/255), colour = rgb(52/255, 106/255, 141/255), alpha=0.5) +
    facet_grid(  ~ `Data Set`, scales = "free") + 
      theme_bw() +
      scale_color_viridis_d()
  }

  
bac <- mkplot(d, "Bacteria")  + theme(axis.text.x = element_text(angle = 45, hjust = 1)) + theme(axis.title.x = element_blank()) 
fungi <- mkplot(d, "Fungi") + theme(axis.text.x = element_text(angle = 45, hjust = 1)) + theme(axis.title.x = element_blank())
#fungi <- mkplot(d, "Fungi")
oom <- mkplot(d, "Oomycete")

#bac_eft3 #| 
bac / fungi / oom + plot_layout(guides = "collect") + plot_annotation(tag_levels = 'A')
```


## Convolution Heatmaps

An advantage of CNNs relative to other deep neural networks is their relative interpretability. A CNN can be analysed and activation maps extracted which highlight the regions of the input to which the CNN is most strongly relying on to classify. To examine the responses of the models, we ran all sequence data sets back through the CNN-LSTM models for each taxon and extracted the network activations using the GRAD-CAM method. The profiles were smoothed using FFT and examined, (see Figure \@ref(fig:activations) ). All the models showed a peak of activation at the N-terminus of the sequences, coincident with expected positions of secretion signals. The fungal sequences created a single broad activation region with a width of 50 to 100 amino acids while the bacterial and oomycete sequences create a some smaller grouped peaks in a broader region which were each around 20 amino acids. We examined further the sequences under the largest peaks, specifically for bacterial sequences we used amino acids 25 to 50, for fungal sequences we used amino acids 35 to 80 and for oomycete sequences we used amino acids 15 to 40. Compositional analysis of the sequence under the peaks showed no apparent primary sequence conservation or motifs as shown in the logo plots in Fig \@ref(fig:activations) even within a taxon and data set. Enrichment analysis of different amino acid categories showed some statistically significant ($p < 0.05$) changes in proportions of amino acid types relative to the whole set of sequences in the activated regions (Table \@ref(tab:phyperaatype) ). Bacterial effectors are depleted of hydrophobic amino acids and enriched in polar and basic amino acids.
Fungal effectors are enriched in basic amino acids, with no other differences. The oomycete effectors are most interesting in that their activating regions are depleted in hydrophobic and polar amino acids while enriched in neutral and acidic amino acids.

```{r activations, fig.height=10, fig.width=10, fig.cap="Grad-CAM activation profiles of CNN-LSTM models  when run with all datasets; and sequence logo plots of sequence in the activation region within the dotted lines for effector data (left panel) and non-effector data (right panel). A: Models trained on bacterial sequences; B: Models trained on fungal sequences; C: Models trained on oomycete sequences."}
# knitr::include_graphics("figures/convolution_maps_fixed.pdf")

all_bacteria <- rbind(
  data.table::fread("data/raw/bacteria_training.csv"),
  data.table::fread("data/raw/bacteria_validation.csv"),
  data.table::fread("data/raw/bacteria_testing.csv")
)

all_oomycete <- rbind(
  data.table::fread("data/raw/oomycete_training.csv"),
  data.table::fread("data/raw/oomycete_validation.csv"),
  data.table::fread("data/raw/oomycete_testing.csv")
)

all_fungi <- rbind(
  data.table::fread("data/raw/fungi_training.csv"),
  data.table::fread("data/raw/fungi_validation.csv"),
  data.table::fread("data/raw/fungi_testing.csv")
)

bacteria_heatmap <- data.table::fread("data/raw/bacteria_heatmap.csv")
fungi_heatmap <- data.table::fread("data/raw/fungi_heatmap.csv")
oomycete_heatmap <- data.table::fread("data/raw/oomycete_heatmap.csv")

bacteria_eff_seqlogo <- get_seqlogo(
  all_bacteria %>% dplyr::filter(label == 1),
  method_plot = "bits",
  start_pos = 25,
  end_pos = 50,
  break_size = 5
)

bacteria_non_eff_seqlogo <- get_seqlogo(
  all_bacteria %>% dplyr::filter(label == 0),
  method_plot = "bits",
  start_pos = 25,
  end_pos = 50,
  break_size = 5
)

oomycete_eff_seqlogo <- get_seqlogo(
  all_oomycete %>% dplyr::filter(label == 1),
  method_plot = "bits",
  start_pos = 15,
  end_pos = 40,
  break_size = 5
)

oomycete_non_eff_seqlogo <- get_seqlogo(
  all_oomycete %>% dplyr::filter(label == 0),
  method_plot = "bits",
  start_pos = 15,
  end_pos = 40,
  break_size = 5
)

fungi_eff_seqlogo <- get_seqlogo(
  all_fungi %>% dplyr::filter(label == 1),
  method_plot = "bits",
  start_pos = 35,
  end_pos = 80,
  break_size = 5
)

fungi_non_eff_seqlogo <- get_seqlogo(
  all_fungi %>% dplyr::filter(label == 0),
  method_plot = "bits",
  start_pos = 35,
  end_pos = 80,
  break_size = 5
)

heatmap_linebar_bacteria <- bacteria_heatmap %>%
  dplyr::mutate(index = V1 + 1) %>%
  dplyr::filter(index <= 750) %>%
  dplyr::select(index, heatmap_bacteria_smooth) %>%
  ggplot(aes(x = index, y = heatmap_bacteria_smooth)) +
  geom_line() +
  geom_vline(xintercept = 25, linetype = 'dashed', color = "red") +
  geom_vline(xintercept = 50, linetype = 'dashed',  color = "red") +
  scale_x_continuous(breaks = seq(0, 750, 150)) +
  labs(x = 'Position', y = "Value") +
  ggplot2::labs(tag = "A")

heatmap_linebar_oomycete <- oomycete_heatmap %>%
  dplyr::mutate(index = V1 + 1) %>%
  dplyr::filter(index <= 750) %>%
  dplyr::select(index, heatmap_oomycete_smooth) %>%
  ggplot(aes(x = index, y = heatmap_oomycete_smooth)) +
  geom_line() +
  geom_vline(xintercept = 15, linetype = 'dashed', color = "blue") +
  geom_vline(xintercept = 40, linetype = 'dashed', color = "blue") +
  geom_vline(xintercept = 45, linetype = 'dashed', color = "red") +
  geom_vline(xintercept = 70, linetype = 'dashed', color = "red") +
  scale_x_continuous(breaks = seq(0, 750, 150)) +
  labs(x = 'Position', y = "Value") +
  ggplot2::labs(tag = "C")

heatmap_linebar_fungi <- fungi_heatmap %>%
  dplyr::mutate(index = V1 + 1) %>%
  dplyr::filter(index <= 750) %>%
  dplyr::select(index, heatmap_fungi_smooth) %>%
  ggplot(aes(x = index, y = heatmap_fungi_smooth)) +
  geom_line() +
  geom_vline(xintercept = 35, linetype = 'dashed', color = "blue") +
  geom_vline(xintercept = 80, linetype = 'dashed', color = "blue") +
  scale_x_continuous(breaks = seq(0, 750, 150)) +
  labs(x = 'Position', y = "Value") +
  ggplot2::labs(tag = "B")

seqlogo_patch <- (
  heatmap_linebar_bacteria +
    bacteria_eff_seqlogo + theme(legend.position = "none") +
    bacteria_non_eff_seqlogo + theme(legend.position = "none") +
    plot_layout(nrow = 1,  tag_level = "new")
  ) /
  (
  heatmap_linebar_fungi +
    fungi_eff_seqlogo + theme(legend.position = "none") +
    fungi_non_eff_seqlogo +
      plot_layout(nrow = 1,  tag_level = "new")
  ) /
  (
  heatmap_linebar_oomycete +
    oomycete_eff_seqlogo + theme(legend.position = "none") +
    oomycete_non_eff_seqlogo + theme(legend.position = "none") +
      plot_layout(nrow = 1,  tag_level = "new")
  )

seqlogo_patch
```

```{r, phyperaatype, echo=FALSE, message=FALSE}
source(here::here("functions","check_proportions_aas.R"))
bacteria_result <- join_files("data/raw/bacteria_testing.csv", "data/raw/bacteria_training.csv", "data/raw/bacteria_validation.csv") %>% 
  truncate_seqs(start = 25, stop = 50) %>% 
  do_hyper()
fungi_result <- join_files("data/raw/fungi_testing.csv", "data/raw/fungi_training.csv", "data/raw/fungi_validation.csv") %>% 
  truncate_seqs(start = 35, stop = 80) %>% 
  do_hyper()
oomycete_result <- join_files("data/raw/oomycete_testing.csv", "data/raw/oomycete_training.csv", "data/raw/oomycete_validation.csv") %>% 
  truncate_seqs(start = 15, stop = 40) %>% 
  do_hyper()

r <- bind_rows(list(bacteria_result, fungi_result, oomycete_result))
r$Taxon <- c("Bacteria", "Fungi", "Oomycete") 
#r <- r %>% select(Taxon, Hydrophobic, Polar, Neutral, Basic, Acidic)
r <- r %>% 
  mutate(Hydrophobic = paste0(sprintf("%.2f", prop_hydro), " : ", sprintf("%.2f",eff_prop_hydro), " (", sprintf("%.2f", p_hydro), ")"),
         Polar = paste0(sprintf("%.2f", prop_polar), " : ", sprintf("%.2f ",eff_prop_polar), " (", sprintf("%.2f", p_polar), ")"),
         Neutral = paste0(sprintf("%.2f",prop_neutral), " : ", sprintf("%.2f",eff_prop_neutral), " (", sprintf("%.2f",p_neutral), ")"),
         Basic = paste0(sprintf("%.2f",prop_basic), " : ", sprintf("%.2f",eff_prop_basic), " (", sprintf("%.2f",p_basic), ")"),
         Acidic = paste0(sprintf("%.2f",prop_acidic), " : ", sprintf("%.2f",eff_prop_acidic), " (", sprintf("%.2f",p_acidic), ")")
         ) %>% select(Taxon, Hydrophobic, Polar, Neutral, Basic, Acidic)
apa_table(
  r,
  digits = 3,
  align = 'c',
  font_size = 'footnotesize',
  caption = 'Enrichment analysis of amino acid types. Proportions of each amino acid type in the activation region of All Sequences : Effector Sequences, with (Probability). Probability by hypergeometric test of the observed number of amino acid type in the region of activation in effector sequences in our compiled sequence set relative to a background of the same region in all sequences in that taxon.'
)

```



## Software Implementation

To make the models useful for developers in their own analytic pipelines we have provided an R package that provides a useful interface to the models. The package and installation instructions are available from GitHub [https://ruthkr.github.io/deepredeff](https://ruthkr.github.io/deepredeff) 

<!-- 

#########################################################
## Start code fragments
##
##
#########################################################

-->



# Discussion

We have compiled a set of known experimentally validated effectors from phytopathogenic bacteria, fungi and oomycetes and used them as positive training examples with which to tune a range of CNN classifiers, the sequences are all taken from the database PHI-Base, a manually curated database that aims to search all the literature on pathogen and host interactions [@phibase]. The data in PHI-Base is complete as far as 2013 at the moment, thus the phytopathogen effectors we have collected should be all those shown experimentally to have an effect on a host that have been reported on since that time. We chose this set as we believed that this would give us the most reliable and unbiased set of effectors on which we could train learners. That is not to say that the set itself cannot be biased and that the set does not introduce any bias into the classifications of our learners. Sources of bias in our sequence set include the time limits on what has so far been included in PHI-Base, any effectors known but not reported on in this seven year time period cannot be represented in the models. The species of phytopathogens represented in the set also create bias, the effectors are not selected from species sampled proportionally or randomly but instead are those that trends in effector research over the last seven years have brought to focus. In particular, species that have had genome sequencing projects over this time are over-represented. There may also be some echoes of other methods previously applied, effectors studied experimentally must be identified first as hypothetical effectors, usually with the aid of computational tools whose models are themselves biased towards the sort of sequence that we already know. The effectors in the literature may therefore be enriched with respect to known features and results from classifiers should be interpreted with this in mind. We are not easily able to quantify any bias, but the greater generalisability of our models over the others tested gives us reason to believe that the sequence set trained on was broad enough for good models to be developed.

A common misconception of deep learning models is that training datasets need to be extremely large, this is only half true. In fact, in practice training data need only to be large with respect to the model size. Here we have coupled small data sets (tens or hundreds of training examples) with small models. The layers and architectures of the models presented here are much smaller than those typically used in large machine vision projects for example. Yet the small models remain useful and have predictive power, indicating a definite role for deep learning approaches in the work of bioinformaticians without truly massive datasets.

Training the models proved to be computationally expensive, the architectures used have a large number of parameters and hyperparameters to be optimised and although only a small fraction ^[This fraction depends for every model, the parameter space is shown in table \@ref(tab:possiblecombination)] of the possible hyperparameter and parameter space was explored we still compared `r 3 * 4 * 50 * 5` models. At a run time of around 144 minutes for CNN-LSTM, 57.2 minutes for CNN-GRU, and 45 minutes for both GRU Embedding and LSTM Embedding. So even for relatively small data sets, a large amount of specialised GPU compute power is required. 

The models we created performed exceptionally on the PHI-Base hold-out validation data set of phytopathogenic effector sequences. The greater than 80 % classification accuracy for the fungal and oomycete models is an excellent accuracy on such sequences and our models outperformed the other classifiers tested by large margins on the PHI-Base fungal, no other machine learning method has been reported to have performed as well on phytopathogen sequences. The greater than 97 % accuracy we observed in our model trained on bacterial phytopathogen effectors is also exceptional and similar to what Xue et al [@deept3] showed in human bacterial pathogen effectors. When we evaluated the proportion of effectors each of our and other classifiers could find in data used to create the other classifiers, we found that our bacterial model outperformed the bacterial models again. A slightly different picture emerged when we compared our method with EffectorP 1 and 2 on fungal data. Both versions of Effector P outperformed our model on training data used to create Effector P in the first instance but identical predictions were made by all fungal classifiers on the validation set provided by Effector P 2. This was coincident with a large drop in accuracy from training to validation data by Effector P 1 and 2.  Combined with observations on our PHI-Base data on higher numbers of false positives from Effector P, we conclude that Effector P is over-fitted slightly on its training data and that our model is likely to be more generally accurate. We expect that the tools used in concert will provide very good classification and prediction of fungal effectors. Our model also performed worse on the data used to test the untrained heuristic oomycete RXLR sequence detector EffectR, finding three of six, relative to EffectR's five of six. This is likely due to the six test sequences being exclusively RXLR and our model being trained on effectors in general, the presence of RXLR not being diagnostic of effector-ness in our model means Researchers hoping to find exclusively RXLR containing proteins would be advised to use EffectR, those hoping to find effectors beyond RXLRs may find utility with our oomycete model.

In developing a negative training data set that contained secreted non-effector proteins we hoped to decouple the identification of effector from identification of secreted protein as a proxy for effector. We believe that we have been succesful at this, the models we developed do identify effectors against a background of secreted proteins, indicating that they have some internal representation of what makes an effector different from a secreted peptide. By examining the activation maps of the CNN models, we learned that the maximum activations in the models remains in the N-terminus of the proteins, coincident with the expected positions of secretions signals and is relatively narrow (about 25 amino acids. We also noted that there is no typical primary sequence motif that can be identified, the identity of the amino acids themselves does not seem important. We did find that various categories of amino acid were enriched or depleted significantly in the narrow activation region of effectors, relative to the same area of non-effector secreted proteins. A favouring of functional properties of the amino acids of the effector over the actual identity may be reflective of the many functional types origins of effectors. It may be that the N-terminal activation region in the effectors represents non-canonical secretion-compatible signal in the effector proteins. In order to evolve towards a useful secretion signal from an arbitrary point it may be enough to have a region that satisfies some chemical property that makes it secretable, yet is different enough in effectors to separate them from the secreted proteins by a deep learning algorithm.

Finally, we have made our models available in an R package _deepredeff_ - from 'deep learning prediction of effector'. This can be obtained from GitHub as shown in methods, installation and usage instructions are available in the documentation provided there. The R package allows the user to run the sequences from various sequence format files against a selected model and obtain a _p_-value that each sequence is an effector according to the model. Various summaries and plots are provided for the user when working on large numbers of sequences. The package integrates well with Bioconductor. For those wishing to use Python to make use of our models, we provide the models as HDF5 files from TensorFlow that can be use in that toolkit.





\newpage

# Supplementary Tables

<!-- 

#########################################################
## Start supplementary information
##
##
#########################################################

-->

\beginsupplement

```{r, bacefftable, echo=FALSE, message = FALSE}
apa_table(
  readr::read_tsv("tables/bacterial_effector_count.tab"),
  digits = 0,
  align = 'c',
  caption = 'Bacterial species contributing to the bacterial effector sequence set retrieved from PHI-Base',
  placement = "h"
)
```

```{r, fungiefftable}
apa_table(
  readr::read_tsv("tables/fungal_effector_count.tab"),
  digits = 0,
  align = 'c',
  caption = 'Fungal species contributing to the fungal effector sequence set retrieved from PHI-Base'
)
```

```{r, oomefftable}
apa_table(
  readr::read_tsv("tables/oomycete_effector_count.tab"),
  digits = 0,
  align = 'c',
  caption = 'Oomycete species contributing to the oomycete effector sequence set retrieved from PHI-Base'
)
```

```{r, possiblecombination}
apa_table(
  readr::read_tsv("tables/possible_combination_each_model.tab"),
  digits = 0,
  align = 'c',
  caption = 'List of possible combination of hyperparamaters setting for each model'
)
```

```{r, fixedhyperparam}
# apa_table(
#   readr::read_tsv("tables/fixed_hyperparam.tab"),
#   digits = 0,
#   align = 'c',
#   caption = 'Hyperparameters fixed in automatic scanning of hyperparameter space for each model'
# )

readr::read_tsv("tables/fixed_hyperparam.tab") %>%
  knitr::kable(
    "latex", 
    longtable = T, 
    booktabs = T,
    digits = 0,
    align = 'c',
    caption = 'Hyperparameters fixed in automatic scanning of hyperparameter space for each model'
  ) %>%
  kableExtra::kable_styling(latex_options = c("repeat_header"), font_size = 7)
```

```{r, tunedhyperparam}
# apa_table(
#   readr::read_tsv("tables/tuned_hyperparam.tab"),
#   digits = 0,
#   align = 'c',
#   caption = 'Hyperparameters tuned in manual scanning of hyperparameter space for each model'
# )

readr::read_tsv("tables/tuned_hyperparam.tab") %>%
  knitr::kable(
    "latex", 
    longtable = T, 
    booktabs = T,
    digits = 0,
    align = 'c',
    caption = 'Hyperparameters tuned in manual scanning of hyperparameter space for each model'
  ) %>%
  kableExtra::kable_styling(latex_options = c("repeat_header"), font_size = 7)
```

```{r, hyperparam}
readr::read_tsv("tables/hyperparamscan_info.tab") %>%
  knitr::kable(
    "latex", 
    longtable = T, 
    booktabs = T,
    digits = 0,
    align = 'c',
    caption = 'Hyperparameters used in automatic scanning of hyperparameter space and best performing parameters values for each model'
  ) %>%
  kableExtra::kable_styling(latex_options = c("repeat_header"), font_size = 7)
```

