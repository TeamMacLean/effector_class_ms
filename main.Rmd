---
title             : "Accurate plant pathogen effector protein classification _ab initio_ with **deepredeff**, an ensemble of convolutional neural networks."
shorttitle        : "CNNs for classifying effector proteins"

author: 
  - name          : "Ruth Kristianingsih"
    affiliation   : "1"
    email         : "ruth.kristianingsih@tsl.ac.uk"
  - name          : "Dan MacLean"
    email         : "dan.maclean@tsl.ac.uk"
    address       : "The Sainsbury Laboratory, University of East Anglia, Norwich, UK, NR4 7UH"
    corresponding : yes    # Define only one corresponding author
    affiliation   : "1"

affiliation:
  - id            : "1"
    institution   : "The Sainsbury Laboratory"


abstract: |
  One or two sentences providing a **basic introduction** to the field,  comprehensible to a scientist in any discipline.
  
  Two to three sentences of **more detailed background**, comprehensible  to scientists in related disciplines.
  
  One sentence clearly stating the **general problem** being addressed by  this particular study.
  
  One sentence summarizing the main result (with the words "**here we show**" or their equivalent).
  
  Two or three sentences explaining what the **main result** reveals in direct comparison to what was thought to be the case previously, or how the  main result adds to previous knowledge.
  
  One or two sentences to put the results into a more **general context**.
  
  Two or three sentences to provide a **broader perspective**, readily comprehensible to a scientist in any discipline.
  
  <!-- https://tinyurl.com/ybremelq -->
  
  <!-- keywords          : "keywords" 
  wordcount         : "X" -->

bibliography      : ["r-references.bib"]

floatsintext      : no
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "man"
output            : papaja::apa6_pdf
---

```{r setup, include = FALSE}
library("papaja")
library(magrittr)
```

```{r analysis-preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

# Introduction

Phytopathogens are a major threat to global crop production.  The fungal phytopathogen _Magnoporthe oryzae_ that causes cereal blast is responsible for around 30% of rice production loss and has now emerged as a pandemic problem on wheat [@Nalley:2016jp] The oomycete _Phytophthora infestans_ causes losses of ~6 billion USD to potato production, annually [@haas2009genome]. The bacterium _Ralstonia solanacearum_ has a wide host range and can cause loses of over 30% in potato, banana and groundnut [@Yuliar:2015hp]. The incidences of crop disease are increasing, global climate change and agricultural practice are expanding the geographical range of pathogens and upping the stakes in the evolutionary arms race.  Effector proteins are the shock troops of infection, manipulating the host at the infection interface to the pathogens advantage. Identifying and characterising a pathogen's effector content is a critical first step in understanding diseases and developing resistance,  but effectors are **notoriously difficult to characterise from sequence data**. In most phyla they have only a few easily determined sequence characteristics (some in fungi are cysteine rich or have a MAX motif, some in oomycetes have the RXLR motif or WY fold)  but in many cases no sequence identifiers are known [@Franceschetti:2017gc]. Characterising effectors requires painstaking molecular experimental work and genome-scale approaches have relied on complex computational pipelines with in-built _a priori_ assumptions about what might constitute an effector sequence in the absence of sequence features known to group them [@Sperschneider:2015kb]. To understand infection processes, to provide genome-level understanding of the functions of this important class of genes and to develop future disease resisting crop varieties there is a critical need to identify effectors computationally from genome and protein sequence data.

Deep learning models can uncover cryptic patterns in data. We believe such models can be trained to identify effectors directly from sequence as others have shown it is possible to classify other eukaryote and bacterial nuclear proteins [@Savojardo:2017eq]. In doing so these models encode information about the features it is using to classify that can be extracted to reveal novel sequence features that current techniques cannot. Deep learning models are distinct from other machine learning processes in that pre-selection of important features is far less critical and the models can learn these features unsupervised from training data [@Jurtz:2017cr]. This property removes the need to know which properties of a data set must be examined before data collection begins. 

Convolutional neural networks (CNNs) ...

Recent work by Sperschneider et al [@Sperschneider:2018gc] developed two ensemble-based machine learning models that could identify effectors and predict localisation with > 70% accuracy.  The work shows clearly that there are unknown patterns in the sequence data that a deep learning method could use to classify. 

Here we use combinations of Convolutional Neural Networks (CNNs) that do not rely so heavily on _a priori_ feature selection - only a well annotated data set,  and that can extract novel features of the sequence in an unbiased, more unsupervised manner.  

Deep learning approaches require positive and negative examples from which to learn - here a list of sequences annotated as effector or not. The larger and more accurate the list the more sensitivity a model can obtain. It is critical that training examples are experimentally verified effectors. Much of the effector annotation in public genomics databases is from computational predictions of genomics and therefore unverified hypothetical effectors. A good source of experimentally verified data is in the Molecular Plant Microbe Interactions (MPMI) literature.  The widest ranging manual curation of MPMI papers is being performed as part of the PHI-Base [@Urban:2017gp] database strategy.



# Methods

## Sequence Data Collection

Sequence data were collected from the PHI-Base database version 4.8 [@phibase] by accessing a text dump of the data kindly prepared on request by the PHI-Base team, the file can be accessed at [https://github.com/PHI-base/data/blob/master/releases/phi-base_current.csv](https://github.com/PHI-base/data/blob/master/releases/phi-base_current.csv). The pipeline in Figure \@ref(fig:effectorcollection) outlines the steps used. We filtered plant effector proteins and their taxonomic groups and collected sequences from UniProt Release 2019_05, using the code in [https://github.com/TeamMacLean/ruth-effectors-prediction/blob/master/scripts/r-scripts/getting-data-new/binary-class/0001_first_step_getting_data.Rmd](https://github.com/TeamMacLean/ruth-effectors-prediction/blob/master/scripts/r-scripts/getting-data-new/binary-class/0001_first_step_getting_data.Rmd). We created a correspondingly sized data set of non-effectors with secretion signals originating in species matched to those from which the effectors were drawn. We downloaded sequences for randomly selected proteins matching these criteria from Ensembl databases [@yates_ensembl_2020] : specifically Ensembl Fungi, Protists and Bacteria manually using the BioMart tools [@smedley_biomart_2009]. Since the BioMart tool is not available on Ensembl Bacteria, we downloaded whole proteome protein sequnces from species matched to those from which the effector came using FTP. With these we used SignalP 3.0 [@dyrlov_bendtsen_improved_2004] in order to filter the secreted sequences and selected accordingly. ^[Ruth, what version of the code and what parameter settings did you use?] Redundant sequences were filtered using BLASTp [@camacho_blast_2009]. We achieved these steps using the code in [https://github.com/TeamMacLean/ruth-effectors-prediction/blob/master/scripts/r-scripts/getting-secreted-data/0005_process_signalp_data.Rmd](https://github.com/TeamMacLean/ruth-effectors-prediction/blob/master/scripts/r-scripts/getting-secreted-data/0005_process_signalp_data.Rmd).



```{r, effectorcollection, out.width='50%', fig.align='center', fig.cap="Workflow diagram for collection of effector sequences from the PHI-Base database annotation and cross reference to UniProt"}
knitr::include_graphics(
  "figures/flowchart_effectorsequence_collection.pdf",
)
```

```{r, noneffectorcollection, fig.cap="Workflow diagram for collection of secreted non-effector sequences from Ensembl Bacteria, Fungi, and Protists"}
knitr::include_graphics(
  "figures/flowchart_secreted_noneffectorsequence_collection.pdf",
)
```

## Encoding and Subsetting Sequences

The sequences collected were encoded using either one-hot encoding (CNN-LSTM based models) or integer based encoding (CNN-GRU-LSTM models). Sequences were post-padded with zeroes to bring the vectors to identical lengths to each other and the longest sequence in the taxon data set ^[Ruth, what were the lengths for each model?]. Encoded sequences were split into taxon specific training, test and validation sets at a 60%, 20%, 20% split respectively as described in code at [https://github.com/TeamMacLean/ruth-effectors-prediction/blob/master/scripts/r-scripts/getting-secreted-data/0008_split_and_encode.Rmd](https://github.com/TeamMacLean/ruth-effectors-prediction/blob/master/scripts/r-scripts/getting-secreted-data/0008_split_and_encode.Rmd)


## Model Training

We trained four model types on each taxon specific sequence set: CNN-LSTM, CNN-GRU, LSTM-Embedding,  GRU-Embedding. We trained each model using a basic random hyperparameter setting initialisation step followed by All training was performed in R 3.5.3, Tensorflow 1.12.0, Keras 2.2.4, Python 3.6.9 on a MacBook Pro (Retina, 13-inch, Early 2013) with an Intel Core i7-3450M (3.00GHz) processor and 8 GB (1600 MHz DDR3) of RAM.  

### Hyperparameter Scans

We performed a random search of hyperparameter effects by selecting 50 of all possible hyperparameters depending on each model type at random. The total of possible hyperparameter combination for each model can be found in Table \@ref(tab:possiblecombination). Additionally, the detail list of hyperparamaters that were fixed and tuned can be found in Table \@ref(tab:fixedhyperparam) and \@ref(tab:tunedhyperparam), respectively. We applied five-fold cross validation at each combination and recorded model performance on the validation data at each iteration. Code for this can be found in [https://github.com/TeamMacLean/ruth-effectors-prediction/tree/master/scripts/python-scripts/hyperparameter-scan-scripts](https://github.com/TeamMacLean/ruth-effectors-prediction/tree/master/scripts/python-scripts/hyperparameter-scan-scripts). Final selected hyperparameter settings for models in each species group can be seen in Table \@ref(tab:hyperparam).  

### Fine tuning

With best performing models from hyperparameter scans we carried out a process of fine tuning manually to ensure that models did not over- or underfit data and were trained to be as general as possible. We loaded best performing models and assessed changes in selected parameters as described in Results to produce final models. These processes were performed manually using Python scripts that can be found in [https://github.com/TeamMacLean/ruth-effectors-prediction/tree/master/scripts/python-scripts/manual_tune_scripts](https://github.com/TeamMacLean/ruth-effectors-prediction/tree/master/scripts/python-scripts/manual_tune_scripts). The results were visualised using R and the codes are stored in [https://github.com/TeamMacLean/ruth-effectors-prediction/tree/master/scripts/r-scripts/model-scripts/multi-class/secreted_data](https://github.com/TeamMacLean/ruth-effectors-prediction/tree/master/scripts/r-scripts/model-scripts/multi-class/secreted_data). Once each model was tuned accordingly using the training and test sets, final classification testing and estimation of accuracy was done with the held-out validation set that the model had not seen until this point. The final models were saved into HDF5 objects and stored in the repository at [https://github.com/TeamMacLean/ruth-effectors-prediction/tree/master/data/final_model_hdf5](https://github.com/TeamMacLean/ruth-effectors-prediction/tree/master/data/final_model_hdf5).

### Model Classification Correlation

We calculated correlations between the classifications from best performing models on the hold-out test data set using Pearson's correlation co-efficient on the 1/0 classification vectors. 

## Ensemble Functions

We computed an aggregate classification using two different ensemble functions, weighted average and an overall majority option. 
  
Weighted average is computed as

\begin{equation}
\bar{y} = \frac{\sum_{i=1}^{n} w_i y_i}{\sum_{i=1}^{n} w_i} ,
\end{equation}
  
where $w_i$ is the weight, $y_i$ is the prediction value of the $i^{th}$ model, and $n$ is the total number of model. In our case, we use the accuracy of each model as the average. 

Overall majority is computed as
  
\begin{equation}
\tilde{y} = argmax(x_1, x_2, ..., x_n) ,
\end{equation}
where 
  
\begin{equation}
y =
   \begin{cases}
          1 & \text{for } \tilde{y} > 0.5 \\
          0 & \text{for } \tilde{y} \leq 0.5
  \end{cases}
\end{equation}


## Metrics

We used the following calculations for different accuracy metrics in our evaluations, specifically: accuracy, sensitivity, specificity. $TP$, $TN$, $FP$, and $FN$ refer to the number of true positives, true negatives, false positives and false negatives, respectively. 

Accuracy ($Acc$) is the ratio between correctly classified non-effectors and effectors and all samples:
\begin{equation}
  Acc = \frac{TP + TN}{TP + TN + FP + FN}
\end{equation}

Sensitivity $Sn$ is the ratio between correctly predicted as effectors and all effectors:
\begin{equation}
  Sn = \frac{TP}{TP + FN}
\end{equation}

Specificity ($Sp$) is the ratio between correctly predicted as non-effectors and all non-effectors:
\begin{equation}
  Sp = \frac{TN}{TN + FP}
\end{equation}

# Results

## Sequence Collection

The performance of the trained classifiers is dependent on the quality of the input training data, so it was important that we collected as high a quality set of annotated effectors as possible. To this end we used PHI-Base [@phibase] as our primary sequence origin. Sequences in PHI-Base are human curated from the literature and have therefore been noted in experimental studies. They do not derive from large scale annotations or contain hypothetical or predicted proteins. This attribute makes it ideal for our purposes as the effectors in PHI-Base are those that have been specifically reported as such in the published literature and are not of the class of sequences that are merely suspected of being effectors on the basis of carrying a secretion signal. To collect effector sequences we parsed a whole database text dump of version 4.8 [https://github.com/PHI-base/data](https://github.com/PHI-base/data/blob/master/releases/phi-base_current.csv), all proteins marked as plant pathogen effectors were filtered and we used the IDs and UniProt IDs to collect the protein sequences from PHI-Base or UniProt if PHI-Base stored only the ID. The sequences and IDs retrieved can be seen in the data file in this manuscript's repository [https://github.com/TeamMacLean/ruth-effectors-prediction/blob/master/data/getting-data-new/binary-class-data/effector_data.csv](https://github.com/TeamMacLean/ruth-effectors-prediction/blob/master/data/getting-data-new/binary-class-data/effector_data.csv). Effector sequences were then divided into taxonomic groups as bacterial, fungal or oomycete derived accordingly. The total number of plant effectors per group can be seen in Table \@ref(tab:effsummary). The species and effector count in each group can be seen in Tables \@ref(tab:bacefftable), \@ref(tab:fungiefftable) and \@ref(tab:oomefftable)

Sequences for non-effector, secreted proteins were collected using a similar pipeline. Randomly selected proteins from each species carrying secretion signals were extracted from Ensembl databases using the BioMart tool. For each species noted in Tables \@ref(tab:bacefftable), \@ref(tab:fungiefftable) and \@ref(tab:oomefftable) we collected an identical number of non-effector, secreted proteins to that collected in the effector set from either the same strain or species. This gave us a balanced data set of effector proteins as positive learning examples and non-effector secreted proteins as negative learning examples. Figure \@ref(fig:noneffectorcollection) summarises the process of building the non-effector set, and the full set of sequences and IDs retrieved can be seen in the following data file [https://github.com/TeamMacLean/ruth-effectors-prediction/tree/master/data/secreted_data](https://github.com/TeamMacLean/ruth-effectors-prediction/tree/master/data/secreted_data). 


## Model Selection and Training

We selected four base architectures on which to build models for learning. Two of these contained Convolutional Neural Network (CNN) layers followed by either a Long Short Term Memory Layer (LSTM) or a Gated Recurrent Unit (GRU), two contained an Embedding Layer followed by the LSTM or GRU. All models had fully-connected dense layers after this. See Figure \@ref(fig:modelarch). Hyperparameter values for the models were identified using a random search as described in methods. Model variants within the hyperparameter search were assessed by comparing accuracy values on the development validation fraction of the training data. For each model type and taxon training data combination we selected the hyperparameter set giving highest accuracy on the validation set for manual tuning.  Final selected hyperparameter settings for models in each taxon can be seen in Table \@ref(tab:hyperparam), ^[Ruth, what were the actual best accuracy and loss values for the hyperparamter scan models selected and in Table \@ref(tab:hyperparam) , this table should have a column that shows this] a total of twelve models were produced, one for each taxon and model combination. 

We then manually ran and checked the accuracy and loss of the models on the training and validation sets to investigate instances of overfitting and assess generality. Smaller models are less likely to overfit data, so we investigated the effect of regularization rate, filter count and kernel size on the progress and accuracy of the model as we reduced the size. We found that by reducing the number of kernels on all models from 2 to 1 and the number of filters reduced from 32 to 16 we removed apparent overfitting and retained high accuracy, with training completing in 40 epochs. Final training progressions for each model in each taxon can be seen in Figure \@ref(fig:trainingprogress). We tested the finalised models on the hold-out test fraction of the data that had not been previously seen, for the four bacterial sequence trained models we had accuracies in the range 93.4 % to 97.4 %, for the four fungal models we observed accuracy in the range 60.5 % to 84.2 % and for the four oomycete models we saw accuracy from 64.7 % to 82.3 %, reported in table \@ref(tab:overall_model). All the models we generated had high statistical power and can accurately and reliably classify effectors from other secreted proteins in that taxon. 

## Model Characteristics

We examined the tendency of the models to call false positives or false negatives preferentially by creating confusion matrices of the classifications relative to the ground truth on the hold-out test data.\@ref(fig:confusionfinalmodels) The bacterial sequence trained models in general showed high accuracy and only one or two miscalls with no error bias except for the GRU-Embedding model which called five from 38 effectors as non effectors. The fungal sequence trained models were less accurate overall and showed a small amount more bias, again in the GRU-Embedding model, which was biased towards calling effectors as non-effectors and the CNN-LSTM model which was slightly biased in the opposite direction, calling non-effectors as effectors. The oomycete models were again quite balanced but the GRU-Embedding model showed a quite conservative tendancy calling 12 out of 17 effectors as non-effectors whilst getting all 17 non-effectors correct. Overall the models are accurate and show little to no bias with the exception of the GRU-Embedding type. In oomycete sequences in particular and in this class of model across the different sequence types showed itself to tend to call real effectors as not of that class. 

Classification correlations between the different model architectures were high and positive in the bacterial sequence trained model's calls, in the range 0.8 - 0.88, see Figure \@ref(fig:model_corrs). CNN-GRU and LSTM-Embedding showed identical prediction sets. We saw similar levels of correlation  in the CNN-LSTM, GRU-Embedding and CNN-GRU fungal sequence trained model, in the range 0.79 - 0.9; though there was a significantly lower range of correlations with the LSTM-Embedding which were in the range 0.36 - 0.51. The models trained on oomycete sequences all showed this lower range of correlations, in the range 0.3 - 0.65. The higher correlation across bacterial trained models is likely from a mixture of the larger training set size and a greater uniformity of the sequences themselves. For the fungal sequence trained models we can see that the LSTM-Embedding model does not perform as well as the others. The oomycete sequence trained models all show a lower range correlation reflecting the likely less uniform and smaller training set. It is clear that, particularly for the fungal and oomycete models, each architecture is capturing separate aspects of the sequences and classifying on those with slightly varying levels of success.  

## Ensemble Models

We examined the usefulness of combining the predictions of the different model architectures using an ensemble function that takes the vectors of classifications of each model architecture as input. We performed the classification of the hold-out test data set using the ensembled models and the results can be seen in Table \@ref(tab:ensembleresults). With the models trained in bacterial sequences we saw an increase in classification accuracy over the best model, up to 0.99 for both ensemble functions. However, with the fungal and oomycete models we saw decreases relative to the best single model in both cases due to the higher accuracy of the CNN-LSTM model being diluted by the combined inacuracy of the other model architectures. The 




## Comparison With Other Classification Software
Given the accuracies of the above we selected the ensemble bacterial model and the CNN-LSTM fungal and oomycete models to evaluate the performance of our models against widely used effector identification software

^[Ruth, can you confirm that this is accurate for the rest of the analysis - are these the combinations of models you are using: You will need to discuss this with me before we make any changes to this document]



## Convolution Heatmaps

## Software Implementation

```{r, effsummary}
apa_table(
  readr::read_tsv("tables/effectors_by_path_group.tab"),
  digits = 0,
  align = 'c',
  caption = 'Effectors found in three major pathogen groups'
)
```

```{r, bacefftable}
apa_table(
  readr::read_tsv("tables/bacterial_effector_count.tab"),
  digits = 0,
  align = 'c',
  caption = 'Bacterial species contributing to the bacterial effector sequence set retrieved from PHI-Base'
)
```

```{r, fungiefftable}
apa_table(
  readr::read_tsv("tables/fungal_effector_count.tab"),
  digits = 0,
  align = 'c',
  caption = 'Fungal species contributing to the fungal effector sequence set retrieved from PHI-Base'
)
```

```{r, oomefftable}
apa_table(
  readr::read_tsv("tables/oomycete_effector_count.tab"),
  digits = 0,
  align = 'c',
  caption = 'Oomycete species contributing to the oomycete effector sequence set retrieved from PHI-Base'
)
```

```{r, possiblecombination}
apa_table(
  readr::read_tsv("tables/possible_combination_each_model.tab"),
  digits = 0,
  align = 'c',
  caption = 'List of possible combination of hyperparamaters setting for each model'
)
```

```{r, fixedhyperparam}
apa_table(
  readr::read_tsv("tables/fixed_hyperparam.tab"),
  digits = 0,
  align = 'c',
  caption = 'Hyperparameters fixed in automatic scanning of hyperparameter space for each model'
)
```

```{r, tunedhyperparam}
apa_table(
  readr::read_tsv("tables/tuned_hyperparam.tab"),
  digits = 0,
  align = 'c',
  caption = 'Hyperparameters tuned in automatic scanning of hyperparameter space for each model'
)
```

```{r, hyperparam }
 readr::read_tsv("tables/hyperparamscan_info.tab") %>% 
  knitr::kable( "latex", longtable = T, booktabs = T,
           digits = 0,
  align = 'c',
  caption = 'Hyperparameters used in automatic scanning of hyperparameter space and best performing parameters values for each model'
         ) %>%
  kableExtra::kable_styling(latex_options = c("repeat_header"), font_size = 7) 

```

```{r, ensembleresults}
apa_table(
  readr::read_tsv("tables/ensemble_model_performance.tab"),
  digits = 3,
  align = 'c',
  caption = 'Final performance of Ensemble models in each taxon on the unseen, hold-out test data fraction of sequences.'
)
```

```{r, modelarch, fig.cap="Overview of model architectures tested in this analysis, A: CNN-LSTM model; B: CNN-GRU model; C:LSTM-Embedding model; D:GRU-Embedding model"}
knitr::include_graphics("figures/model_arch.png")
```

```{r, trainingprogress, fig.cap="Training trajectories showing Accuracy and Loss over 40 epochs for four model types on A: Bacterial sequence training set; B: Fungal sequence training set; C: Oomycete sequence training set"}
knitr::include_graphics("figures/final_train_progress.png")
```

```{r, overallmodel}
apa_table(
  readr::read_tsv("tables/final_model_performance.tab"),
  digits = 3,
  align = 'c',
  caption = 'Final performance of models in each taxon on the unseen, hold-out test data fraction of sequences.'
)
```

```{r, confusionfinalmodels, fig.cap="Confusion matrices showing tendency of models to call false positive or false negative errors on the final test data. A: Models trained on bacterial sequences; B: Models trained on fungal sequences; C: Models trained on oomycete sequences."}

knitr::include_graphics("figures/final_confusion.png")

```

```{r, modelcorrs, fig.cap="Pearson correlations between classifications made by different models; A: Models trained on bacterial sequences; B: Models trained on fungal sequences; C: Models trained on oomycete sequences."}
knitr::include_graphics("figures/drafts.png")
```



# Discussion


\newpage

# References
```{r create_r-references}
r_refs(file = "r-references.bib")
```

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id="refs" custom-style="Bibliography"></div>
\endgroup
